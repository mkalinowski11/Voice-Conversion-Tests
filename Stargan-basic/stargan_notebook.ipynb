{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDRgBR_3iPEt"
      },
      "source": [
        "# Libs & Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3M9eRduiS4m",
        "outputId": "6f1fff30-c389-42fd-ee1e-00ff5ecd5d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "from torch.utils.data import DataLoader\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N6pQu4ijiSHa"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model, optimizer, filename, lr, device):\n",
        "    checkpoint = torch.load(filename, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    current_epoch = checkpoint[\"current_epoch\"]\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "    return current_epoch\n",
        "\n",
        "def save_checkpoint(model, optimizer, config, filename=\"my_checkpoint.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"current_epoch\": config.current_epoch\n",
        "    }\n",
        "    torch.save(checkpoint, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAyGhvO0boII"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G34tfpkXTW2z"
      },
      "outputs": [],
      "source": [
        "class Voice_Dataset(Dataset):\n",
        "    def __init__(self, source_voice_path, target_voice_path):\n",
        "        self.source_voice_spects = os.path.join(source_voice_path, \"speaker1\", \"spects\")\n",
        "        self.source_voice_embeds = os.path.join(source_voice_path, \"speaker1\", \"embeddings\")\n",
        "        self.target_voice_spects = os.path.join(target_voice_path, \"speaker2\", \"spects\")\n",
        "        self.target_voice_embeds = os.path.join(target_voice_path, \"speaker2\", \"embeddings\")\n",
        "        #\n",
        "        self.source_voices_len = len(os.listdir(self.source_voice_spects))\n",
        "        self.target_voices_len = len(os.listdir(self.target_voice_spects))\n",
        "        #\n",
        "        self.dataset_length = min(self.source_voices_len, self.target_voices_len)\n",
        "        self.idxs = [*range(self.dataset_length)]\n",
        "        random.shuffle(self.idxs)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.dataset_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      src_voice_spect = torch.load(\n",
        "          os.path.join(self.source_voice_spects, f\"spect{index}.pth\")\n",
        "      )\n",
        "      src_voice_embed = torch.load(\n",
        "          os.path.join(self.source_voice_embeds, f\"embed{index}.pth\")\n",
        "      )\n",
        "      trg_voice_spect = torch.load(\n",
        "          os.path.join(self.target_voice_spects, f\"spect{index}.pth\")\n",
        "      )\n",
        "      trg_voice_embed = torch.load(\n",
        "          os.path.join(self.target_voice_embeds, f\"embed{index}.pth\")\n",
        "      )\n",
        "      return src_voice_spect, src_voice_embed, trg_voice_spect, trg_voice_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "naGsuMjVacGU"
      },
      "outputs": [],
      "source": [
        "src_voice_path = \"/content/gdrive/MyDrive/processed_data/\"\n",
        "target_voice_path = \"/content/gdrive/MyDrive/processed_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI6QLI1XaXLq",
        "outputId": "db88c790-2f59-453f-91c8-ff45d74a65e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 80, 128]) torch.Size([1, 256]) torch.Size([1, 80, 128]) torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "dataset = Voice_Dataset(src_voice_path, target_voice_path)\n",
        "train_dataloader = DataLoader(dataset, batch_size=1)\n",
        "train_dataloader_iter = iter(train_dataloader)\n",
        "src_spect, src_embed, trg_spect, trg_embed = next(train_dataloader_iter)\n",
        "print(\n",
        "    src_spect.shape,\n",
        "    src_embed.shape,\n",
        "    trg_spect.shape,\n",
        "    trg_embed.shape\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V2akvHKbqHg"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1LziCKchcQCr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from fastai.layers import init_linear\n",
        "\n",
        "class CIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_out, embed_dim):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.gamma = nn.Linear(embed_dim, dim_out)\n",
        "        init_linear(self.gamma)\n",
        "        self.beta = nn.Linear(embed_dim, dim_out)\n",
        "        init_linear(self.beta)\n",
        "    \n",
        "    def forward(self, x, embed):\n",
        "        sigma, mu = torch.std_mean(x, dim=2, keepdim=True)\n",
        "        sigma = torch.clamp(sigma, min=1e-7)\n",
        "        gamma = self.gamma(embed)[..., None]\n",
        "        beta = self.gamma(embed)[..., None]\n",
        "        return gamma * (x - mu) / sigma + beta\n",
        "\n",
        "\n",
        "class ConditioningBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_in, dim_out, kernel_size, stride, padding, embed_dim):\n",
        "        super(ConditioningBlock, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv1d(in_channels=dim_in, out_channels=dim_out, \n",
        "            kernel_size=kernel_size, stride=stride, padding=padding, bias=True\n",
        "        )\n",
        "        self.cin = CIN(dim_out, embed_dim)\n",
        "        self.glu = nn.GLU(dim=1)\n",
        "\n",
        "    def forward(self, x, embed):\n",
        "        x = self.conv(x)\n",
        "        x = self.cin(x, embed)\n",
        "        x = self.glu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6bz7r8Zsbrxn"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        #if device is None:\n",
        "        #    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        #else:\n",
        "        #    self.device = device\n",
        "\n",
        "        # speaker embed\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_map = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim*2, 256),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        init_linear(self.embed_map[0])\n",
        "\n",
        "        self.init_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(5, 15), stride=(1, 1), padding=(2, 7)),\n",
        "            nn.GLU(dim=1)\n",
        "        )\n",
        "\n",
        "        dims = [64, 128, 256]\n",
        "        block = []\n",
        "        for i in range(1, len(dims)):\n",
        "            cur, nxt = dims[i-1], dims[i]\n",
        "            block.append(nn.Conv2d(in_channels=cur, out_channels=nxt*2, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True))\n",
        "            block.append(nn.InstanceNorm2d(num_features=nxt*2, affine=True))\n",
        "            block.append(nn.GLU(dim=1))\n",
        "        self.down_sample = nn.Sequential(*block)\n",
        "\n",
        "        self.down_converse = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=5120, out_channels=256, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm1d(num_features=256, affine=True)\n",
        "        )\n",
        "\n",
        "        self.cond_1 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_2 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_3 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_4 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_5 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_6 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_7 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_8 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "        self.cond_9 = ConditioningBlock(dim_in=256, dim_out=512, kernel_size=5, stride=1, padding=2, embed_dim=self.embed_dim)\n",
        "\n",
        "        self.up_converse = nn.Conv1d(in_channels=256, out_channels=5120, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        self.up_sample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=1024, kernel_size=5, stride=1, padding=2, bias=True),\n",
        "            nn.PixelShuffle(2), # channels / 4\n",
        "            nn.GLU(dim=1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=512, kernel_size=5, stride=1, padding=2, bias=True),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.GLU(dim=1),\n",
        "        )\n",
        "\n",
        "        self.out_layer = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=(5, 15), stride=(1, 1), padding=(2, 7), bias=True)\n",
        "\n",
        "\n",
        "    # x:   (bs, 80, width)\n",
        "    # src: (256,)\n",
        "    # trg: (256,)\n",
        "    def forward(self, x, src, trg):\n",
        "        x = x.to(self.device)\n",
        "        src = src.to(self.device)\n",
        "        trg = trg.to(self.device)\n",
        "\n",
        "        bs, _, width = x.shape\n",
        "        src_trg = torch.cat([src, trg], dim=1)\n",
        "        src_trg = self.embed_map(src_trg)\n",
        "        # initialize layer\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.init_layer(x)\n",
        "        \n",
        "        # down sampling layer\n",
        "        x = self.down_sample(x)\n",
        "        \n",
        "        # down conversion layer\n",
        "        x = x.contiguous().view(bs, 5120, width // 4)\n",
        "\n",
        "        x = self.down_converse(x)\n",
        "        \n",
        "        # bottleneck layer\n",
        "        x = self.cond_1(x, src_trg)\n",
        "        x = self.cond_2(x, src_trg)\n",
        "        x = self.cond_3(x, src_trg)\n",
        "        x = self.cond_4(x, src_trg)\n",
        "        x = self.cond_5(x, src_trg)\n",
        "        x = self.cond_6(x, src_trg)\n",
        "        x = self.cond_7(x, src_trg)\n",
        "        x = self.cond_8(x, src_trg)\n",
        "        x = self.cond_9(x, src_trg)\n",
        "        \n",
        "        # up conversion layer\n",
        "        x = self.up_converse(x)\n",
        "        x = x.view(bs, 256, 20, width // 4)\n",
        "\n",
        "        # up sampling layer\n",
        "        x = self.up_sample(x)\n",
        "        \n",
        "        # output layer\n",
        "        x = self.out_layer(x)\n",
        "        \n",
        "        return x.view(bs, 80, width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBOA9T_Mbyvk",
        "outputId": "a91efc0b-4e68-4cd5-ab5b-97c7ee0add82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 128])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "genr = Generator(embed_dim = 256, device=None)\n",
        "random_tensor = torch.randn(1, 80, 128)\n",
        "embed = torch.randn(1, 256)\n",
        "result = genr(random_tensor, embed, embed)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R11d1bNKcCj9"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_rbuP-uFcLJ-"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        #if device is None:\n",
        "        #    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        #else:\n",
        "        #    self.device = device\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.input_dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.init_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.GLU(dim=1)\n",
        "        )\n",
        "\n",
        "        dims = [64, 128, 256, 512]\n",
        "        block = []\n",
        "        for i in range(1, len(dims)):\n",
        "            cur, nxt = dims[i-1], dims[i]\n",
        "            block.append(nn.Conv2d(in_channels=cur, out_channels=nxt*2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True))\n",
        "            block.append(nn.InstanceNorm2d(num_features=nxt*2, affine=True))\n",
        "            block.append(nn.GLU(dim=1))\n",
        "        block.append(nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=True))\n",
        "        block.append(nn.InstanceNorm2d(num_features=1024, affine=True))\n",
        "        block.append(nn.GLU(dim=1))\n",
        "        self.down_sample = nn.Sequential(*block)\n",
        "\n",
        "        self.linear = nn.Linear(in_features=512*10*16, out_features=1)\n",
        "\n",
        "        self.embed_map_src = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 256),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.embed_map_trg = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 256),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.embed = nn.Linear(self.embed_dim*2, 512)\n",
        "        init_linear(self.embed_map_src[0])\n",
        "        init_linear(self.embed_map_trg[0])\n",
        "        init_linear(self.embed)\n",
        "\n",
        "    def forward(self, x, src, trg, dropout=False):\n",
        "        x = x.to(self.device)\n",
        "        src = src.to(self.device)\n",
        "        trg = trg.to(self.device)\n",
        "\n",
        "        bs, _, width = x.shape\n",
        "\n",
        "        src = self.embed_map_src(src)\n",
        "        trg = self.embed_map_trg(trg)\n",
        "        src_trg = torch.cat([src, trg], dim=1)\n",
        "        embed = self.embed(src_trg)\n",
        "\n",
        "        # input drop out\n",
        "        if dropout:\n",
        "            x = self.input_dropout(x)\n",
        "\n",
        "        # init layer\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.init_layer(x)\n",
        "        \n",
        "        # down sampling layer\n",
        "        x = self.down_sample(x)\n",
        "        \n",
        "        # global sum pooling\n",
        "        h = torch.sum(x, dim=(-1, -2))\n",
        "        x = self.linear(x.view(-1, 512*10*16))\n",
        "        \n",
        "        y = x + (embed[:, None]@h[..., None]).squeeze(-1)\n",
        "        return y.view(bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzExQLFOcRLo",
        "outputId": "ec73d2e5-c052-41e3-c04c-69eb3f2aea67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-10.6861], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "discr = Discriminator(embed_dim = 256, device=None)\n",
        "random_tensor = torch.randn(1, 80, 128)\n",
        "embed = torch.randn(1, 256)\n",
        "result = discr(random_tensor, embed, embed)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvHlWt8diDby"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qJTAnQaScXt2"
      },
      "outputs": [],
      "source": [
        "src_voice_path = \"/content/gdrive/MyDrive/processed_data/\"\n",
        "target_voice_path = \"/content/gdrive/MyDrive/processed_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NOlDO56HUdFf"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.resume_path_gen = \"/content/gdrive/MyDrive/test/generator2.pth\"\n",
        "        self.resume_path_dis = \"/content/gdrive/MyDrive/test/discriminator2.pth\"\n",
        "        self.save_model_path = \"/content/gdrive/MyDrive/test\"\n",
        "        self.optimizers = {\n",
        "            \"gen_lr\": 0.0001,\n",
        "            \"dis_lr\": 0.00005,\n",
        "            \"beta1\": 0.9,\n",
        "            \"beta2\": 0.999\n",
        "        }\n",
        "        self.hparam = {\n",
        "        \"a\": 1,\n",
        "        \"b\": 0,\n",
        "        \"lambda_id\": 5,\n",
        "        \"lambda_cyc\": 10\n",
        "        }\n",
        "        self.num_epochs = 1001\n",
        "        self.epoch_save = 2\n",
        "        self.current_epoch = 0\n",
        "        self.gen_freq = 5\n",
        "        self.batch_size = 1\n",
        "        self.num_workers = 0\n",
        "        self.load_checkpoint = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5BAGtVlqU1Lx"
      },
      "outputs": [],
      "source": [
        "def train(config, source_voice_path, target_voice_path):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataset = Voice_Dataset(source_voice_path = source_voice_path,\n",
        "                  target_voice_path = target_voice_path)\n",
        "    train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    gen = Generator(embed_dim=256).to(device)\n",
        "    dis = Discriminator(embed_dim=256).to(device)\n",
        "\n",
        "    gen_lr = config.optimizers['gen_lr']\n",
        "    dis_lr = config.optimizers['dis_lr']\n",
        "    beta1 = config.optimizers['beta1']\n",
        "    beta2 = config.optimizers['beta2']\n",
        "\n",
        "    gen_opt = torch.optim.Adam(gen.parameters(), gen_lr, [beta1, beta2])\n",
        "    dis_opt = torch.optim.Adam(dis.parameters(), dis_lr, [beta1, beta2])\n",
        "    \n",
        "    hparam = config.hparam\n",
        "    l1_loss = nn.L1Loss()\n",
        "    l2_loss = nn.MSELoss()\n",
        "\n",
        "    if config.load_checkpoint:\n",
        "        print(\"downloaded weights\")\n",
        "        # generator\n",
        "        config.current_epoch = load_checkpoint(gen, gen_opt, config.resume_path_gen, config.optimizers['gen_lr'], device)\n",
        "        # discriminator\n",
        "        _ = load_checkpoint(dis, dis_opt, config.resume_path_dis, config.optimizers['dis_lr'], device)\n",
        "    print(f\"starting from epoch {config.current_epoch}\")\n",
        "    for epoch in range(config.current_epoch, config.num_epochs):\n",
        "        loop = tqdm(train_loader, leave=True)\n",
        "        for idx, (src_spect , src_embed, trg_spect , trg_embed) in enumerate(loop):\n",
        "            src_spect = src_spect.to(device)\n",
        "            src_embed = src_embed.to(device)\n",
        "            trg_spect = trg_spect.to(device)\n",
        "            trg_embed = trg_embed.to(device)\n",
        "            # gen inference\n",
        "            x_src_src = gen(src_spect, src_embed, src_embed)\n",
        "            x_src_trg = gen(src_spect, src_embed, trg_embed)\n",
        "            x_src_trg_src = gen(x_src_trg, trg_embed, src_embed)\n",
        "            # discriminator\n",
        "            d_src = dis(src_spect, src_embed, trg_embed)\n",
        "            d_src_trg = dis(x_src_trg, trg_embed, src_embed)\n",
        "            #\n",
        "            dis_loss = torch.mean((d_src_trg - hparam['b']) ** 2 + (d_src - hparam['a']) ** 2)\n",
        "            # reset grad discriminator\n",
        "            dis_opt.zero_grad()\n",
        "            dis_loss.backward(retain_graph=True)\n",
        "            dis_opt.step()\n",
        "            if idx % config.gen_freq == 0:\n",
        "                id_loss = l2_loss(src_spect, x_src_src)\n",
        "                cyc_loss = l1_loss(src_spect, x_src_trg_src)\n",
        "                d_src_trg_2 = dis(x_src_trg, trg_embed, src_embed)\n",
        "                adv_loss = torch.mean((d_src_trg_2 - hparam['a']) ** 2)\n",
        "                gen_loss = hparam['lambda_id'] * id_loss + hparam['lambda_cyc'] * cyc_loss + adv_loss\n",
        "                gen_opt.zero_grad()\n",
        "                gen_loss.backward(retain_graph=True)\n",
        "                gen_opt.step()\n",
        "                metrics = dis_loss.item(), gen_loss.item(), adv_loss.item()\n",
        "                loop.set_postfix({'dis loss':metrics[0], 'gen loss':metrics[1], 'adv loss':metrics[2]})\n",
        "        if (epoch % config.epoch_save == 0) and (epoch != 0):\n",
        "            save_checkpoint(gen, gen_opt, config, os.path.join(config.save_model_path, f\"generator{epoch}.pth\"))\n",
        "            save_checkpoint(dis, dis_opt, config, os.path.join(config.save_model_path, f\"discriminator{epoch}.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "yz5srJCcU5R2",
        "outputId": "0e401956-b57f-42ab-dbea-e3ed43277eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloaded weights\n",
            "starting from epoch 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/1132 [00:08<42:03,  2.24s/it, dis loss=325, gen loss=7.68e+3, adv loss=7.46e+3]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5bddfe93c568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_voice_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_voice_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-85f025f1595b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, source_voice_path, target_voice_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_spect\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msrc_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_spect\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrg_embed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0msrc_spect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_spect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0msrc_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-41f1e5bfaa26>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m       )\n\u001b[1;32m     22\u001b[0m       src_voice_embed = torch.load(\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_voice_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"embed{index}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       )\n\u001b[1;32m     25\u001b[0m       trg_voice_spect = torch.load(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mread_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "train(config, src_voice_path, target_voice_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPr7Uvp_Xe5s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oAyGhvO0boII",
        "5V2akvHKbqHg",
        "R11d1bNKcCj9"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
